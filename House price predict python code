import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
import seaborn as sns
from scipy.stats import skew
from scipy.stats import kurtosis
from scipy import stats

train_data=pd.read_csv("D:/CLASS/Term 2/machine learning 2/assignment 1/house-prices-advanced-regression-techniques/train.csv",
                      index_col="Id")
test_data=pd.read_csv("D:/CLASS/Term 2/machine learning 2/assignment 1/house-prices-advanced-regression-techniques/test.csv",
                      index_col="Id")
                      
# Exploratory Data Analysis
## In order to reduce the number of variables, we performed the Spearman correlation coefficient test. 

cor_matrix=pd.DataFrame(train_data.select_dtypes(include="integer").corr(method="spearman"))
cor_matrix.tail()

##In order to filter the numerical variables that are highly correlated with our target variable - SalePrice, we set the threshold of coefficient at 0.6. Then, we dropped the variables we want to use for feature extraction later from the relatively weakly correlated coefficients list, which is the "delete_2".

keep_1=cor_matrix.loc[abs(cor_matrix["SalePrice"])>=0.6,"SalePrice"].index
delete_2=cor_matrix.drop(["MSSubClass","PoolArea","MiscVal","YearBuilt","YrSold",'WoodDeckSF', 'OpenPorchSF',
       'EnclosedPorch', '3SsnPorch', 'ScreenPorch']).loc[abs(cor_matrix["SalePrice"])<0.6,"SalePrice"].index
delete_2
corrmat = train_data.corr(method="spearman")
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True)
## Here we used the list that contains the variables correlated with SalePrice (keep_1) to plot a correlation heatmap. 
cor_plot_data=cor_matrix.loc[abs(cor_matrix["SalePrice"])>=0.6,keep_1]
ax = sns.heatmap(
    cor_plot_data, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True,
    annot=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)

## In order to avoid applying feature engineering twice, we added the "SalePrice" column to the test set and concatenated the 
##test set with the train set.

test_add=test_data.copy()
test_add["SalePrice"]=pd.Series()
test_add.head()

train_test_merge=pd.concat([train_data,test_add])
train_test_merge.head()

## Now that we have a complete merged dataset, we first removed the columns in the "delete_2" list. Afterwards, GarageArea was removed
##because it is highly correlated with GarageCars, which is also highly correlated with SalePrice. 
train_test_merge=train_test_merge.drop(delete_2 , axis='columns')
train_test_merge=train_test_merge.drop(["GarageArea"], axis='columns')
train_test_merge.head()

train_test_merge=train_test_merge.drop(["MSSubClass","Alley","GarageYrBlt","MasVnrType","MasVnrArea"], axis='columns')
train_test_merge.head()

# Missing Value Imputation
train_test_merge.isnull().sum().sort_values(ascending=False)

##Reading the data description file, we found out that the following variables have NAs, which actually means that such attribute 
##just does not exist in the house. Therefore, we replaced those NAs with zeroes. 
train_test_merge[["PoolQC","MiscFeature","GarageCond","GarageQual","GarageFinish","GarageType","BsmtCond","Fence",\
"FireplaceQu","BsmtQual","BsmtFinType1","BsmtFinType2","BsmtExposure","GarageCars","TotalBsmtSF"]]=\
train_test_merge[["PoolQC","MiscFeature","GarageCond","GarageQual","GarageFinish","GarageType","BsmtCond","Fence","FireplaceQu",\
"BsmtQual","BsmtFinType1","BsmtFinType2","BsmtExposure","GarageCars","TotalBsmtSF"]].fillna(0)

train_test_merge.isna().sum().sort_values(ascending=False)
train_test_merge[["MSZoning","Functional","Utilities","Electrical","SaleType","Exterior2nd","Exterior1st","KitchenQual"]]=\
train_test_merge[["MSZoning","Functional","Utilities","Electrical","SaleType","Exterior2nd","Exterior1st","KitchenQual"]].\
apply(lambda x:x.fillna(x.value_counts().index[0]))
train_test_merge.isna().sum().sort_values(ascending=False)
plt.scatter(train_data["Neighborhood"],train_data["LotFrontage"])
train_test_merge["LotFrontage"]=train_test_merge.groupby("Neighborhood")["LotFrontage"].transform(lambda x:x.fillna(x.median()))
train_test_merge.isna().sum().sort_values(ascending=False)

# Remove Outliers
##In order to detect outliers for numerical values, we plotted pairplot of them. As shown in the graphs, there are outliers in SalePrice,
##LotFrontage, GrLivArea, and TotalBsmtSF. Although it seems that MiscVal and PoolArea have outliers, this is not true as approximatel
##90% of their values are 0; thus, outliers not applicable. 
sns.set()
cols = ['SalePrice','LotFrontage', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF',  'YearBuilt','MiscVal','PoolArea']
sns.pairplot(train_test_merge[0:1460][cols], height = 2.5)
plt.show()

##We used the IQR score to remove outliers. We set the outlier size parameter as 4, which removed 14 rows in total.
Q1 = train_test_merge[0:1460][["SalePrice","LotFrontage","GrLivArea","TotalBsmtSF"]].quantile(0.25)
Q3 = train_test_merge[0:1460][["SalePrice","LotFrontage","GrLivArea","TotalBsmtSF"]].quantile(0.75)
IQR = Q3 - Q1
print(IQR,Q1,Q3)

idx = ((train_test_merge[0:1460][["SalePrice","LotFrontage","GrLivArea","TotalBsmtSF"]]>= (Q1 - 4 * IQR)) &\
(train_test_merge[0:1460][["SalePrice","LotFrontage","GrLivArea","TotalBsmtSF"]] <= (Q3 + 4 * IQR))).all(axis=1)
train_test_merge=pd.concat([train_test_merge[0:1460].loc[idx],train_test_merge[1460:]])

# Feature Extraction & Feature Engineering
## In feature extraction, we created 3 features: Age_built, Total_area, and porch_area. 
We created Age_built feature, because the age of a house at the time of the sale is more important for the sale price rather than when it was built or sold. 
With the same logic, we created Total_area and porch_area as the total square feet is more relevant than the individual area size. 
train_test_merge["Age_built"]=train_test_merge["YrSold"]-train_test_merge["YearBuilt"]
train_test_merge["Total_area"]=train_test_merge["GrLivArea"]+train_test_merge["TotalBsmtSF"]
train_test_merge["porch_area"]=train_test_merge["WoodDeckSF"]+train_test_merge["OpenPorchSF"]+train_test_merge["EnclosedPorch"]+\
train_test_merge["3SsnPorch"]+train_test_merge["ScreenPorch"]
train_test_merge=train_test_merge.drop(["YrSold","YearBuilt","GrLivArea","TotalBsmtSF","WoodDeckSF","OpenPorchSF","EnclosedPorch",\
"3SsnPorch","ScreenPorch"],axis=1)

##For numerical values, it is imperative for them to be transformed if they are skewed as they can make our model biased. Below is the 
##skewness values and plots of numerical values including our target variable, SalePrice.
skew(train_test_merge.loc[0:1460][["SalePrice","LotFrontage","MiscVal","Total_area","porch_area"]])
f, axes = plt.subplots(2, 3, figsize=(15, 7), sharex=False,sharey=False)
sns.distplot(train_test_merge.loc[0:1460]["SalePrice"], color="skyblue", ax=axes[0,0])
sns.distplot(train_test_merge.loc[0:1460]["LotFrontage"], color="olive", ax=axes[0,1])
sns.distplot(train_test_merge.loc[0:1460]["MiscVal"], color="green", ax=axes[0,2])
sns.distplot(train_test_merge.loc[0:1460]["Total_area"], color="gold", ax=axes[1,0])
sns.distplot(train_test_merge.loc[0:1460]["porch_area"], color="teal", ax=axes[1,1])
sns.distplot(train_test_merge.loc[0:1460]["GarageCars"], color="blue", ax=axes[1,2])

## Therefore, we transformed the SalePrice with log1p, and porch_area with sqaure root. The skewness of the two 
## variables now has decreased closer to 0.
f, axes = plt.subplots(2, 3, figsize=(15, 7), sharex=False,sharey=False)
sns.distplot(np.log1p(train_test_merge.loc[0:1460]["SalePrice"]), color="skyblue", ax=axes[0,0])
sns.distplot(train_test_merge.loc[0:1460]["LotFrontage"], color="olive", ax=axes[0,1])
sns.distplot(train_test_merge.loc[0:1460]["MiscVal"], color="green", ax=axes[0,2])
sns.distplot(train_test_merge.loc[0:1460]["Total_area"], color="gold", ax=axes[1,0])
sns.distplot(np.sqrt(train_test_merge.loc[0:1460]["porch_area"]), color="teal", ax=axes[1,1])
sns.distplot(train_test_merge.loc[0:1460]["GarageCars"], color="blue", ax=axes[1,2])

skew(np.log1p(train_test_merge.loc[0:1460][["SalePrice"]]))
skew(np.sqrt(train_test_merge.loc[0:1460]["porch_area"]))

train_test_merge[["SalePrice"]]=np.log1p(train_test_merge.loc[0:1460][["SalePrice"]])
train_test_merge[["porch_area"]]=np.sqrt(train_test_merge[["porch_area"]])
train_test_merge.head()

## Dummy Encoding for all remaining categorial variables
df_dummy = pd.get_dummies(train_test_merge.select_dtypes(include="object"))
df_dummy.head()

train_test_with_dummy=train_test_merge.merge(
  df_dummy,
  left_on="Id",
  right_on="Id",
  how="left",
)
train_test_with_dummy.head()

train_test_model1=train_test_with_dummy.select_dtypes(exclude="object")
train_test_model1.head()

# Splitting into Train 1 and Test 1 Set
train_model1=train_test_model1.loc[1:1460,]
test_model1=train_test_model1.loc[1461:,]

# Model Training 1
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

##Our first model was trained using a simple linear regression. 
 X_train, X_val, y_train, y_val= train_test_split(train_model1.drop("SalePrice",axis=1), \
 train_model1["SalePrice"], test_size=0.2, random_state=1)
model1 = LinearRegression().fit(X_train, y_train)
y_pred_val1=model1.predict(X_val)
y_pred_test1=model1.predict(test_model1.drop("SalePrice",axis=1))
y_pred_test1
y_pred_train1=model1.predict(X_train)

# Evaluation
from sklearn import metrics 
print(np.sqrt(metrics.mean_squared_error(y_train, y_pred_train1)))
print(np.sqrt(metrics.mean_squared_error(y_val, y_pred_val1)))
sub1=pd.DataFrame()
sub1["Id"]=test_model1.index
sub1["SalePrice"]=np.expm1(y_pred_test1)
sub1.to_csv('D:/CLASS/Term 2/machine learning 2/assignment 1/submission2.csv',index=False)

# Feature selection for the second model - Chi-Square Test
from scipy.stats import chi2_contingency
import scipy.stats as stats
class ChiSquare:
    def __init__(self, dataframe):
        self.df = dataframe
        self.p = None #P-Value
        self.chi2 = None #Chi Test Statistic
        self.dof = None
        
        self.dfObserved = None
        self.dfExpected = None
        
    def _print_chisquare_result(self, colX, alpha):
        result = ""
        if self.p<alpha:
            result="{0} is IMPORTANT for Prediction".format(colX)
        else:
            result="{0} is NOT an important predictor. (Discard {0} from model)".format(colX)

        print(result)
        
    def TestIndependence(self,colX,colY, alpha=0.05):
        X = self.df[colX].astype(str)
        Y = self.df[colY].astype(str)
        
        self.dfObserved = pd.crosstab(Y,X) 
        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)
        self.p = p
        self.chi2 = chi2
        self.dof = dof 
        
        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)
        
        self._print_chisquare_result(colX,alpha)

#Initialize ChiSquare Class
cT = ChiSquare(train_test_merge.loc[1:1460])

#Feature Selection
testColumns = train_test_merge.loc[1:1460].select_dtypes(include="object").columns
for var in testColumns:
    cT.TestIndependence(colX=var,colY="SalePrice" ) 
    
plt.scatter(train_data["HouseStyle"],train_data["SalePrice"])
train_test_model2=train_test_merge.drop(["Utilities","RoofStyle","Functional","Fence","RoofMatl","GarageCond","Condition1","Condition2","Exterior1st","Exterior2nd","BldgType","BsmtFinType1","BsmtFinType2","MiscVal","HouseStyle","PoolQC"],axis=1)
train_test_model2["LotShape"]=(train_test_model2["LotShape"]=="Reg")*1
train_test_model2["LandSlope"]=(train_test_model2["LandSlope"]=="Gentle slope")*1
train_test_model2["LandContour"]=(train_test_model2["LandContour"]=="Lvl")*1
train_test_model2["PavedDrive"]=(train_test_model2["PavedDrive"]=="Y")*1

train_test_model2["MiscFeature"]=(train_test_model2["MiscFeature"]!=0)*1

train_test_model2["Electrical"]=(train_test_model2["Electrical"]=="SBrkr")*1

qc1_dict = {"Po": 1, "Fa": 2, "TA": 3, "Gd": 4, "Ex": 5}
train_test_model2["HeatingQC"] = train_test_model2["HeatingQC"].map(qc1_dict).astype(int)
train_test_model2["ExterQual"] = train_test_model2["ExterQual"].map(qc1_dict).astype(int)
train_test_model2["ExterCond"] = train_test_model2["ExterCond"].map(qc1_dict).astype(int)
train_test_model2["KitchenQual"] = train_test_model2["KitchenQual"].map(qc1_dict).astype(int)
qc2_dict = {0 : 0,"Po": 1, "Fa": 2, "TA": 3, "Gd": 4, "Ex": 5}
train_test_model2["BsmtQual"] = train_test_model2["BsmtQual"].map(qc2_dict).astype(int)
train_test_model2["BsmtCond"] = train_test_model2["BsmtCond"].map(qc2_dict).astype(int)
train_test_model2["FireplaceQu"] = train_test_model2["FireplaceQu"].map(qc2_dict).astype(int)
train_test_model2["GarageQual"] = train_test_model2["GarageQual"].map(qc2_dict).astype(int)
qc3_dict = {0 : 0,"No": 1, "Mn": 2, "Av": 3, "Gd": 4}
train_test_model2["BsmtExposure"] = train_test_model2["BsmtExposure"].map(qc3_dict).astype(int)

# Another dummy encoding step for the variables after the new feature transformation.

df_dummy2=pd.get_dummies(train_test_model2.select_dtypes(include="object"))
df_dummy.head()

train_test_with_dummy2=train_test_model2.merge(
  df_dummy2,
  left_on="Id",
  right_on="Id",
  how="left",
)

train_test_with_dummy2=train_test_with_dummy2.select_dtypes(exclude="object")
train_test_with_dummy2.head()

# Splitting into Train 2 and Test 2 set

train_model2=train_test_with_dummy2.loc[1:1460,]
test_model2=train_test_with_dummy2.loc[1461:,]

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

 X_train2, X_val2, y_train2, y_val2= train_test_split(train_model2.drop("SalePrice",axis=1), train_model2["SalePrice"], test_size=0.2, random_state=1)

model2 = LinearRegression().fit(X_train2, y_train2)

model2.score(X_train2, y_train2)

model2.score(X_val2, y_val2)

model2.intercept_

model2.coef_

y_pred_val2=model2.predict(X_val2)


y_pred_test2=model2.predict(test_model2.drop("SalePrice",axis=1))
y_pred_test2

y_pred_train2=model2.predict(X_train2)

print(np.sqrt(metrics.mean_squared_error(y_train2, y_pred_train2)))
print(np.sqrt(metrics.mean_squared_error(y_val2, y_pred_val2)))

sub2=pd.DataFrame()
sub2["Id"]=test_model2.index
sub2["SalePrice"]=np.expm1(y_pred_test2)

sub2.to_csv('/Users/Esther/Desktop/IE/Courses/Machine Learning II/Assignments/Assignment 1/submission2.csv',index=False)

# Feature Selection with the third model - PCA

from sklearn.preprocessing import StandardScaler

# Separating out the features
x = train_test_with_dummy2.drop("SalePrice",axis=1)
# Separating out the target
y = train_test_with_dummy2.loc[:,['SalePrice']]
# Standardizing the features
x = StandardScaler().fit_transform(x)

train_test_with_dummy2

from sklearn.decomposition import PCA
pca = PCA(n_components=85) #selecting the number of components
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents,index=train_test_with_dummy2.index)

# Concatenating the PCA table with SalePrice
train_test_model3 = pd.concat([principalDf,train_test_with_dummy2["SalePrice"]],axis=1,ignore_index=True)
train_test_model3.head()

# Spilitting into Train 3 and Test 3 Set

train_model3=train_test_model3.loc[1:1460,]
test_model3=train_test_model3.loc[1461:,]

# Model Training 3

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

 X_train3, X_val3, y_train3, y_val3= train_test_split(train_model3.iloc[:,:-1], train_model3.iloc[:,-1], test_size=0.2, random_state=1)

model3 = LinearRegression().fit(X_train3, y_train3)

model3.score(X_train3, y_train3)

model3.score(X_val3, y_val3)

model3.intercept_

model3.coef_

y_pred_val3=model3.predict(X_val3)


y_pred_test3=model3.predict(test_model3.iloc[:,:-1])
y_pred_test3

y_pred_train3=model3.predict(X_train3)

print(np.sqrt(metrics.mean_squared_error(y_train3, y_pred_train3)))
print(np.sqrt(metrics.mean_squared_error(y_val3, y_pred_val3)))

sub3=pd.DataFrame()
sub3["Id"]=test_model3.index
sub3["SalePrice"]=np.expm1(y_pred_test3)

sub3.to_csv('/Users/Esther/Desktop/IE/Courses/Machine Learning II/Assignments/Assignment 1/submission3.csv',index=False)

# Model Training 4 with Ridge

from sklearn.linear_model import Ridge
from sklearn import linear_model
from sklearn.model_selection import cross_val_score

model4 = Ridge(alpha=0.1)
model4.fit(X_train3, y_train3)

model4.score(X_train3, y_train3)

model4.score(X_val3, y_val3)

y_pred_test4=model4.predict(test_model3.iloc[:,:-1])
y_pred_test4

y_pred_train4=model4.predict(X_train3)
y_pred_val4=model4.predict(X_val3)

print(np.sqrt(metrics.mean_squared_error(y_train3, y_pred_train4)))
print(np.sqrt(metrics.mean_squared_error(y_val3, y_pred_val4)))

sub4=pd.DataFrame()
sub4["Id"]=test_model3.index
sub4["SalePrice"]=np.expm1(y_pred_test4)

sub4.to_csv('/Users/Esther/Desktop/IE/Courses/Machine Learning II/Assignments/Assignment 1/submission4.csv',index=False)

# Model Training 5 with Lasso

model5 = linear_model.Lasso(alpha=0.001) 
#we tried different alpha sizes and concluded that the best alpha is 0.001.
model5.fit(X_train3, y_train3)

model5.score(X_train3, y_train3)

model5.score(X_val3, y_val3)

y_pred_test5=model5.predict(test_model3.iloc[:,:-1])
y_pred_test5

y_pred_train5=model5.predict(X_train3)
y_pred_val5=model5.predict(X_val3)

print(np.sqrt(metrics.mean_squared_error(y_train3, y_pred_train5)))
print(np.sqrt(metrics.mean_squared_error(y_val3, y_pred_val5)))

sub5=pd.DataFrame()
sub5["Id"]=test_model3.index
sub5["SalePrice"]=np.expm1(y_pred_test5)

sub5.to_csv('/Users/Esther/Desktop/IE/Courses/Machine Learning II/Assignments/Assignment 1/submission5.csv',index=False)

# Conclusion

##Our score for the first model submitted was 0.14985. After the feature selection with Chi-Square test and feature transformation,\
it improved to 0.14431. With PCA, the model score improved a little by 0.0011. Finally, we tried both the Ridge and Lasso regression. \
Ridge did not improve our model at all; however, with Lasso, the score increased to 0.14230, which we selected as our final model.




